{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "#---------------------------------------------\n",
    "#Settings: definição dos parêmetros da rede:\n",
    "n_classes = 2                     # A base de dados DogsAndCats tem 2 classes de objetos!!!!\n",
    "nepochs = 10                      # Número de épocas para o treinamento!!!\n",
    "batch_size = 32                   # Número de imagens por batch!!!\n",
    "image_size = 32                   # Todas as imagens devem ser redimensionadas para 32x32 pixels!!!\n",
    "nchannels = 3                     #\n",
    "n_input = image_size * image_size * nchannels # Tamanho da entrada!\n",
    "learning_rate = 1e-3              # Taxa de aprendizado!!!\n",
    "#TRAIN_DIR = 'S:/UTFPR/2018-2/Reconhecimento de Padrões em Imagens HD/dog-cat/train'\n",
    "#TEST_DIR =  'S:/UTFPR/2018-2/Reconhecimento de Padrões em Imagens HD/dog-cat/test'\n",
    "TRAIN_DIR = '../APS02/Dogs vs. Cats/kaggle/train'\n",
    "TEST_DIR = '../APS02/Dogs vs. Cats/kaggle/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#---------------------------------------------\n",
    "def multilayer_perceptron (n_camada_1, n_camada_2, x):\n",
    "    #n_camada_1 = 256 # Número de neurônios na camada 1!!!\n",
    "    #n_camada_2 = 128 # Número de neurônios na camada 2!!!\n",
    "    \n",
    "    # Primeira camada da rede:\n",
    "    i = 1\n",
    "    W1 = tf.get_variable('w'+str(i), [n_input, n_camada_1], initializer = tf.random_normal_initializer())\n",
    "    b1 = tf.get_variable('b'+str(i), [n_camada_1], initializer = tf.random_normal_initializer())\n",
    "    y1 = tf.nn.sigmoid(tf.matmul(x, W1) + b1) #tf.nn.relu() or tf.matmul(x, W1) + b1!!!! \n",
    "    \n",
    "    # Segunda camada da rede:\n",
    "    i = i+1\n",
    "    W2 = tf.get_variable('w'+str(i), [n_camada_1, n_camada_2], initializer = tf.random_normal_initializer())\n",
    "    b2 = tf.get_variable('b'+str(i), [n_camada_2], initializer = tf.random_normal_initializer())\n",
    "    y2 = tf.nn.sigmoid(tf.matmul(y1, W2) + b2) \n",
    "    \n",
    "    # Última camada da rede:\n",
    "    i = i+1\n",
    "    W3 = tf.get_variable('w'+str(i),[n_camada_2, n_classes], initializer = tf.random_normal_initializer())\n",
    "    b3 = tf.get_variable('b'+str(i),[n_classes], initializer = tf.random_normal_initializer())\n",
    "    out_layer = tf.matmul(y2, W3) + b3 \n",
    "\n",
    "    return out_layer\n",
    "\n",
    "#---------------------------------------------\n",
    "def create_label(image_name):\n",
    "    word_label = image_name.split('.')[-3]\n",
    "    if word_label == 'cat':\n",
    "        return np.array([1,0])\n",
    "    elif word_label == 'dog':\n",
    "        return np.array([0,1])\n",
    "    else: \n",
    "        print (\"Esta classe não existe!!!!!\")\n",
    "\n",
    "#---------------------------------------------\n",
    "def read_dataset (filename):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for img in tqdm(os.listdir(filename)):\n",
    "        path = os.path.join(filename, img)\n",
    "        img_data = cv2.imread(path)\n",
    "        #img_data = cv2.imread(path, cv2.IMREAD_GRAYSCALE) #converte para níveis de cinza\n",
    "        img_data = cv2.resize(img_data, (image_size, image_size)) #deixa com as dimensões definidas\n",
    "        \n",
    "        #primeira forma de normalização\n",
    "        #min_val = np.min(img_data)\n",
    "        #max_val = np.max(img_data )\n",
    "        #img_data = (img_data-min_val)/(max_val-min_val)\n",
    "        #segunda forma de normalização\n",
    "        #img_data = img_data/255.0    \n",
    "        \n",
    "        #cria os vetores de dados e de labels\n",
    "        X.append(np.array(img_data))\n",
    "        Y.append(np.array(create_label(img)))\n",
    "        \n",
    "    return X,Y\n",
    "\n",
    "#---------------------------------------------\n",
    "def next_batch (num, data, labels):\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = data[idx]\n",
    "    labels_shuffle = labels[idx]\n",
    "    return data_shuffle, labels_shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:42<00:00, 585.20it/s]\n",
      "  0%|          | 0/12500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7855773dc327>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mY_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_dataset\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTEST_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ff568722db98>\u001b[0m in \u001b[0;36mread_dataset\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m#cria os vetores de dados e de labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ff568722db98>\u001b[0m in \u001b[0;36mcreate_label\u001b[0;34m(image_name)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#---------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mword_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mword_label\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cat'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Leitura da base de dados:\n",
    "X_train,Y_train = read_dataset (TRAIN_DIR)\n",
    "X_train = np.asarray(X_train).reshape(-1, n_input)\n",
    "Y_train = np.asarray(Y_train)\n",
    "\n",
    "X_test,Y_test = read_dataset (TEST_DIR)\n",
    "X_test = np.asarray(X_test).reshape(-1, n_input)\n",
    "Y_test = np.asarray(Y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/programas/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#terceira forma de normalização\n",
    "#ATENÇÃO: para testar, comente todas as formas de normalização da função read_dataset()\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)  \n",
    "X_train = scaler.transform(X_train)  \n",
    "X_test = scaler.transform(X_test)  \n",
    "\n",
    "# Variáveis do tensorflow:\n",
    "Y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "X = tf.placeholder(tf.float32, [None, image_size * image_size * nchannels])\n",
    "\n",
    "Ypred = multilayer_perceptron (256, 128, X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/programas/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "('Epoch: ', ' 1')\n",
      "  training loss:\t\t1.254891\n",
      "  validation accuracy:\t\t61.76 %\n",
      "('Epoch: ', ' 2')\n",
      "  training loss:\t\t0.812838\n",
      "  validation accuracy:\t\t67.32 %\n",
      "('Epoch: ', ' 3')\n",
      "  training loss:\t\t0.674030\n",
      "  validation accuracy:\t\t70.52 %\n",
      "('Epoch: ', ' 4')\n",
      "  training loss:\t\t0.594986\n",
      "  validation accuracy:\t\t72.87 %\n",
      "('Epoch: ', ' 5')\n",
      "  training loss:\t\t0.541673\n",
      "  validation accuracy:\t\t74.75 %\n",
      "('Epoch: ', ' 6')\n",
      "  training loss:\t\t0.491321\n",
      "  validation accuracy:\t\t77.25 %\n",
      "('Epoch: ', ' 7')\n",
      "  training loss:\t\t0.471740\n",
      "  validation accuracy:\t\t77.90 %\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Leitura da base de dados:\n",
    "    #X_train,Y_train = read_dataset (TRAIN_DIR)\n",
    "    #X_train = np.asarray(X_train).reshape(-1, n_input)\n",
    "    #Y_train = np.asarray(Y_train)\n",
    "\n",
    "    #X_test,Y_test = read_dataset (TEST_DIR)\n",
    "    #X_test = np.asarray(X_test).reshape(-1, n_input)\n",
    "    #Y_test = np.asarray(Y_test)\n",
    "\n",
    "    #terceira forma de normalização\n",
    "    #ATENÇÃO: para testar, comente todas as formas de normalização da função read_dataset()\n",
    "    #scaler = StandardScaler()  \n",
    "    #scaler.fit(X_train)  \n",
    "    #X_train = scaler.transform(X_train)  \n",
    "    #X_test = scaler.transform(X_test)  \n",
    "\n",
    "    # Variáveis do tensorflow:\n",
    "    #Y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "    #X = tf.placeholder(tf.float32, [None, image_size * image_size * nchannels])\n",
    "   \n",
    "    #Ypred = multilayer_perceptron (X) \n",
    "  \n",
    "    # Funções de custo:\n",
    "    error1 = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(tf.nn.softmax(Ypred)), reduction_indices = [1]))\n",
    "    error2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Ypred, labels = Y)) \n",
    "    error3 = tf.reduce_mean(tf.reduce_sum(tf.square(tf.nn.softmax(Ypred) - Y), reduction_indices = [1]))\n",
    "    error = error1\n",
    "    \n",
    "    # Funções para minimização de erro: \n",
    "    optimizer1 = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(error)\n",
    "    optimizer2 = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(error)\n",
    "    optimizer = optimizer2\n",
    " \n",
    "    corr = tf.equal(tf.argmax(Ypred,1),tf.argmax(Y,1))\n",
    " \n",
    "    accuracy = tf.reduce_mean(tf.cast(corr,tf.float32))\n",
    "\n",
    "    # Inicialização de variáveis:\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        # Treino:\n",
    "        for epoch in range(nepochs):\n",
    "            train_err = 0\n",
    "            train_acc = 0\n",
    "            train_batches = 0\n",
    "            total_batch = int(len(X_train)/batch_size)\n",
    "            for i in range(total_batch):\n",
    "                batch_xs, batch_ys = next_batch (batch_size, X_train, Y_train)\n",
    "                sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "                err, acc = sess.run([error,accuracy], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "                train_err += err\n",
    "                train_acc += acc\n",
    "                train_batches += 1\n",
    "            print(\"Epoch: \", '%2d' % (epoch+1))\n",
    "            print(\"  training loss:\\t\\t{:.6f}\".format(train_err/train_batches))\n",
    "            print(\"  validation accuracy:\\t\\t{:.2f} %\".format(train_acc/train_batches * 100))\n",
    "\n",
    "        # Testes:\n",
    "        test_err = 0\n",
    "        test_acc = 0\n",
    "        test_batches = 0\n",
    "        total_batch = int(len(X_test)/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = next_batch (batch_size, X_test, Y_test)\n",
    "            sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            err, acc = sess.run([error,accuracy], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            test_err += err\n",
    "            test_acc += acc\n",
    "            test_batches += 1\n",
    "        print(\"Final results:\")\n",
    "        print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err/test_batches))\n",
    "        print(\"  test accuracy:\\t\\t{:.2f} %\".format((test_acc/test_batches)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Se precisar modificar o Percepetron, precisa rodar essa célula.\n",
    "# O Jupyter não permite criar de novo ws e bs com mesmo nome.\n",
    "#tf.reset_default_graph() #Descomente para rodar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer 1 Error 1\n",
    "Epoch:   1\n",
    "  training loss:\t\t1.881264\n",
    "  validation accuracy:\t\t48.50 %\n",
    "Epoch:   2\n",
    "  training loss:\t\t1.677853\n",
    "  validation accuracy:\t\t49.78 %\n",
    "Epoch:   3\n",
    "  training loss:\t\t1.532538\n",
    "  validation accuracy:\t\t50.17 %\n",
    "Epoch:   4\n",
    "  training loss:\t\t1.425723\n",
    "  validation accuracy:\t\t51.44 %\n",
    "Epoch:   5\n",
    "  training loss:\t\t1.354925\n",
    "  validation accuracy:\t\t51.47 %\n",
    "Epoch:   6\n",
    "  training loss:\t\t1.265790\n",
    "  validation accuracy:\t\t51.97 %\n",
    "Epoch:   7\n",
    "  training loss:\t\t1.220608\n",
    "  validation accuracy:\t\t51.88 %\n",
    "Epoch:   8\n",
    "  training loss:\t\t1.200131\n",
    "  validation accuracy:\t\t51.47 %\n",
    "Epoch:   9\n",
    "  training loss:\t\t1.140760\n",
    "  validation accuracy:\t\t52.38 %\n",
    "Epoch:  10\n",
    "  training loss:\t\t1.110857\n",
    "  validation accuracy:\t\t52.72 %\n",
    "Final results:\n",
    "  test loss:\t\t\t1.121007\n",
    "  test accuracy:\t\t51.15 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer 1 Error 2\n",
    "Epoch:   1\n",
    "  training loss:\t\t1.647909\n",
    "  validation accuracy:\t\t52.02 %\n",
    "Epoch:   2\n",
    "  training loss:\t\t1.593837\n",
    "  validation accuracy:\t\t51.65 %\n",
    "Epoch:   3\n",
    "  training loss:\t\t1.471031\n",
    "  validation accuracy:\t\t53.42 %\n",
    "Epoch:   4\n",
    "  training loss:\t\t1.425581\n",
    "  validation accuracy:\t\t52.68 %\n",
    "Epoch:   5\n",
    "  training loss:\t\t1.365321\n",
    "  validation accuracy:\t\t52.90 %\n",
    "Epoch:   6\n",
    "  training loss:\t\t1.329543\n",
    "  validation accuracy:\t\t52.51 %\n",
    "Epoch:   7\n",
    "  training loss:\t\t1.283104\n",
    "  validation accuracy:\t\t51.92 %\n",
    "Epoch:   8\n",
    "  training loss:\t\t1.233297\n",
    "  validation accuracy:\t\t53.20 %\n",
    "Epoch:   9\n",
    "  training loss:\t\t1.191547\n",
    "  validation accuracy:\t\t53.08 %\n",
    "Epoch:  10\n",
    "  training loss:\t\t1.175796\n",
    "  validation accuracy:\t\t53.02 %\n",
    "Final results:\n",
    "  test loss:\t\t\t1.188918\n",
    "  test accuracy:\t\t52.15 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer 1 Error 3\n",
    "Epoch:   1\n",
    "  training loss:\t\t0.846617\n",
    "  validation accuracy:\t\t49.85 %\n",
    "Epoch:   2\n",
    "  training loss:\t\t0.837694\n",
    "  validation accuracy:\t\t50.39 %\n",
    "Epoch:   3\n",
    "  training loss:\t\t0.814092\n",
    "  validation accuracy:\t\t51.69 %\n",
    "Epoch:   4\n",
    "  training loss:\t\t0.815053\n",
    "  validation accuracy:\t\t51.14 %\n",
    "Epoch:   5\n",
    "  training loss:\t\t0.806016\n",
    "  validation accuracy:\t\t51.47 %\n",
    "Epoch:   6\n",
    "  training loss:\t\t0.811737\n",
    "  validation accuracy:\t\t51.21 %\n",
    "Epoch:   7\n",
    "  training loss:\t\t0.800126\n",
    "  validation accuracy:\t\t51.90 %\n",
    "Epoch:   8\n",
    "  training loss:\t\t0.794531\n",
    "  validation accuracy:\t\t52.09 %\n",
    "Epoch:   9\n",
    "  training loss:\t\t0.787382\n",
    "  validation accuracy:\t\t52.15 %\n",
    "Epoch:  10\n",
    "  training loss:\t\t0.786845\n",
    "  validation accuracy:\t\t52.24 %\n",
    "Final results:\n",
    "  test loss:\t\t\t0.785623\n",
    "  test accuracy:\t\t51.95 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer 2 Error 1\n",
    "Epoch:   1\n",
    "  training loss:\t\t0.813724\n",
    "  validation accuracy:\t\t56.84 %\n",
    "Epoch:   2\n",
    "  training loss:\t\t0.664701\n",
    "  validation accuracy:\t\t59.29 %\n",
    "Epoch:   3\n",
    "  training loss:\t\t0.659137\n",
    "  validation accuracy:\t\t59.96 %\n",
    "Epoch:   4\n",
    "  training loss:\t\t0.652205\n",
    "  validation accuracy:\t\t61.00 %\n",
    "Epoch:   5\n",
    "  training loss:\t\t0.648369\n",
    "  validation accuracy:\t\t61.60 %\n",
    "Epoch:   6\n",
    "  training loss:\t\t0.650465\n",
    "  validation accuracy:\t\t61.08 %\n",
    "Epoch:   7\n",
    "  training loss:\t\t0.641353\n",
    "  validation accuracy:\t\t62.17 %\n",
    "Epoch:   8\n",
    "  training loss:\t\t0.645863\n",
    "  validation accuracy:\t\t62.09 %\n",
    "Epoch:   9\n",
    "  training loss:\t\t0.638951\n",
    "  validation accuracy:\t\t62.90 %\n",
    "Epoch:  10\n",
    "  training loss:\t\t0.636826\n",
    "  validation accuracy:\t\t62.80 %\n",
    "Final results:\n",
    "  test loss:\t\t\t0.657422\n",
    "  test accuracy:\t\t60.85 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer 2 Error 2\n",
    "Epoch:   1\n",
    "  training loss:\t\t0.740862\n",
    "  validation accuracy:\t\t56.33 %\n",
    "Epoch:   2\n",
    "  training loss:\t\t0.662209\n",
    "  validation accuracy:\t\t59.67 %\n",
    "Epoch:   3\n",
    "  training loss:\t\t0.655810\n",
    "  validation accuracy:\t\t60.35 %\n",
    "Epoch:   4\n",
    "  training loss:\t\t0.650997\n",
    "  validation accuracy:\t\t61.06 %\n",
    "Epoch:   5\n",
    "  training loss:\t\t0.646508\n",
    "  validation accuracy:\t\t61.89 %\n",
    "Epoch:   6\n",
    "  training loss:\t\t0.649950\n",
    "  validation accuracy:\t\t61.12 %\n",
    "Epoch:   7\n",
    "  training loss:\t\t0.641638\n",
    "  validation accuracy:\t\t62.33 %\n",
    "Epoch:   8\n",
    "  training loss:\t\t0.644665\n",
    "  validation accuracy:\t\t61.67 %\n",
    "Epoch:   9\n",
    "  training loss:\t\t0.642716\n",
    "  validation accuracy:\t\t62.20 %\n",
    "Epoch:  10\n",
    "  training loss:\t\t0.639696\n",
    "  validation accuracy:\t\t62.65 %\n",
    "Final results:\n",
    "  test loss:\t\t\t0.645239\n",
    "  test accuracy:\t\t61.80 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer 2 Error 3\n",
    "Epoch:   1\n",
    "  training loss:\t\t0.558197\n",
    "  validation accuracy:\t\t58.94 %\n",
    "Epoch:   2\n",
    "  training loss:\t\t0.472880\n",
    "  validation accuracy:\t\t59.74 %\n",
    "Epoch:   3\n",
    "  training loss:\t\t0.458055\n",
    "  validation accuracy:\t\t61.67 %\n",
    "Epoch:   4\n",
    "  training loss:\t\t0.462267\n",
    "  validation accuracy:\t\t61.09 %\n",
    "Epoch:   5\n",
    "  training loss:\t\t0.457469\n",
    "  validation accuracy:\t\t61.60 %\n",
    "Epoch:   6\n",
    "  training loss:\t\t0.454209\n",
    "  validation accuracy:\t\t62.14 %\n",
    "Epoch:   7\n",
    "  training loss:\t\t0.454985\n",
    "  validation accuracy:\t\t62.10 %\n",
    "Epoch:   8\n",
    "  training loss:\t\t0.450691\n",
    "  validation accuracy:\t\t62.47 %\n",
    "Epoch:   9\n",
    "  training loss:\t\t0.452379\n",
    "  validation accuracy:\t\t62.67 %\n",
    "Epoch:  10\n",
    "  training loss:\t\t0.450615\n",
    "  validation accuracy:\t\t62.89 %\n",
    "Final results:\n",
    "  test loss:\t\t\t0.456251\n",
    "  test accuracy:\t\t61.93 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3im",
   "language": "python",
   "name": "py3im"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
